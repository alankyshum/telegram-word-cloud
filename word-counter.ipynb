{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('.venv': venv)",
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "f92e86e2354309bcb59a98d7fad2aae92ff21b5d58d87da4df92bd3eb2f22066"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import Telegram JSON file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('./data/telegram_export.json', 'r') as export_file:\n",
    "    messages = json.load(export_file)['messages']\n",
    "    messages_df = pd.json_normalize(data=messages)"
   ]
  },
  {
   "source": [
    "## Tokenised words grouped by senders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "from pymaybe import maybe\n",
    "import re\n",
    "\n",
    "analyzer = PerceptronLexicalAnalyzer()\n",
    "\n",
    "# split string into string[] by its words\n",
    "def tokenise_words(body):\n",
    "    cleaned_body = maybe(' '.join(map(lambda w: w.upper(), re.findall('\\w+', body)))).or_else('')\n",
    "    tokenised_chinese_words = analyzer.analyze(cleaned_body).toWordArray()\n",
    "    word_list = []\n",
    "    \n",
    "    try:\n",
    "        for word in tokenised_chinese_words:\n",
    "            if re.search('[a-z]+', word, re.IGNORECASE):\n",
    "                # english words\n",
    "                for eng_word in word.split():\n",
    "                    if len(eng_word) < 2:\n",
    "                        # meaningless words\n",
    "                        continue\n",
    "\n",
    "                    word_list.append(eng_word)\n",
    "            elif re.search('\\w+', word, re.IGNORECASE): \n",
    "                if len(word) < 2:\n",
    "                        # meaningless words\n",
    "                        continue\n",
    "                # all words\n",
    "                word_list.append(word)\n",
    "            # not matching punctuation marks\n",
    "    except UnicodeDecodeError:\n",
    "        print(body)\n",
    "\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "def extract_message(message, combined_word = ''):\n",
    "    if type(message) is str:\n",
    "        return combined_word + message\n",
    "    elif type(message) is list:\n",
    "        for message_segment in message:\n",
    "            return combined_word + extract_message(message_segment)\n",
    "    elif 'text' in message:\n",
    "        return combined_word + extract_message(message['text'])\n",
    "    else:\n",
    "        raise message + 'cannot be extracted'\n",
    "\n",
    "# returns summary[sender][word] = count object\n",
    "# only those with word count > 5 will be returned\n",
    "def summarise_words(words_df, min_count = 5, max_count = 1000, file = None):\n",
    "    summary = {}\n",
    "    combined_words = {}\n",
    "    filtered_summary = {}\n",
    "\n",
    "    for index, message in ten_messages_df.iterrows():\n",
    "        sender = message['from']\n",
    "        body = extract_message(message)\n",
    "        combined_words[sender] = combined_words[sender] + ' ' + body if sender in combined_words else ''\n",
    "    \n",
    "    for sender, combined_word in combined_words.items():\n",
    "        tokenised_words = tokenise_words(combined_word)\n",
    "\n",
    "        for tokenised_word in tokenised_words:\n",
    "            summary[sender] = summary[sender] if sender in summary else {}\n",
    "            summary[sender][tokenised_word] = summary[sender][tokenised_word] if tokenised_word in summary[sender] else 0\n",
    "            summary[sender][tokenised_word] += 1;\n",
    "\n",
    "    for sender in summary:\n",
    "        filtered_summary[sender] = {}\n",
    "        for word, count in sorted(summary[sender].items(), key=lambda item: item[1], reverse=True):\n",
    "            if count >= min_count and count <= max_count:\n",
    "                if file is not None:\n",
    "                    file.write('{sender},{word},{count}\\n'.format(sender = sender, word = word, count = count))\n",
    "                else:\n",
    "                    filtered_summary[sender][word] = count\n",
    "            \n",
    "    return filtered_summary if file is None else file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_file = open('output.csv', 'w')\n",
    "init_file.write('')\n",
    "init_file.close()\n",
    "\n",
    "output_file = open('output.csv', 'a')\n",
    "ten_messages_df = messages_df\n",
    "once_a_day = 720\n",
    "once_a_week = 100\n",
    "word_summary = summarise_words(\n",
    "    ten_messages_df,\n",
    "    min_count = once_a_week,\n",
    "    max_count = once_a_day,\n",
    "    file = output_file\n",
    ")\n",
    "output_file.close()"
   ]
  },
  {
   "source": [
    "- TODO https://www.google.com/search?q=pandas+dataframe+map+column+values&oq=pandas+dataframe+map&aqs=chrome.2.69i57j0l7.10384j0j4&sourceid=chrome&ie=UTF-8\n",
    "- Map `messages[0].text` to tokens using `HanLp`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Top 10 words from each sender"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}